---
layout: post
title: "Interpretations of Probability and the Axiomatic Resolution"
date: 2025-11-14
mathjax: true
---

# Interpretations of Probability and the Axiomatic Resolution

## 1. Compact survey of main interpretations

**1.1 Classical (Laplace).** When a finite experiment has finitely many equally possible outcomes \(\Omega\), for an event \(A\subseteq\Omega\)

\[P(A)=\frac{|A|}{|\Omega|}\]

Use: symmetric games. Failure: requires an ill‑justified equiprobability assumption and does not generalize naturally to infinite or continuous spaces.

**1.2 Frequentist (Long‑run frequency).** Probability of event \(A\) is the limiting relative frequency across independent repeated trials:

\[P(A)=\lim_{n\to\infty}\frac{N_n(A)}{n}\]

when the limit exists. Use: inference from repeatable processes. Failure: does not assign probabilities to single‑case statements or parameters; depends on an idealized limit.

**1.3 Bayesian (Subjective / Degree of belief).** Probability quantifies rational belief about propositions or parameters. Update rule: Bayes' theorem

\[P(\theta\mid D)\propto P(D\mid\theta)P(\theta)\]

Use: coherent updating and decision making. Objection: choice of prior introduces subjectivity.

**1.4 Propensity interpretation.** Probability is a dispositional property of a physical setup (a tendency to produce an outcome). Use: single‑case probabilities tied to physical devices. Objection: metaphysical and hard to test directly.

**1.5 Geometric (Measure‑intuition).** Probability equals relative geometric size (length, area, volume) under a uniform rule. This is a concrete case of a measure on a continuous space.

**1.6 Personalist / Betting (Dutch‑book).** Probability corresponds to coherent betting odds; coherence (avoidance of Dutch books) forces probabilities to satisfy the usual arithmetic constraints.

Each interpretation supplies constructions or semantics that produce numerical assignments; they differ on *what makes* those numbers meaningful.

## 2. Why the axiomatic approach (Kolmogorov) resolves conceptual friction

Kolmogorov's axioms treat probability as a mathematical measure. They are:

Let \((\Omega,\mathcal{F})\) be a measurable space and \(P:\mathcal{F}\to[0,1]\) a function satisfying:

1. **Nonnegativity:** \(P(A)\ge0\) for all \(A\in\mathcal{F}\).
2. **Normalization:** \(P(\Omega)=1\).
3. **Countable additivity:** If \(A_i\in\mathcal{F}\) are pairwise disjoint, then

\[P\Big(\bigcup_{i=1}^\infty A_i\Big)=\sum_{i=1}^\infty P(A_i)\]

Consequences of this stance:

- **Neutrality.** Axioms are silent about interpretation. Any model (classical, frequentist limit, Bayesian prior, geometric measure, propensity) becomes an implementation of a probability measure when it satisfies the axioms.

- **Compatibility.** Conflicts between interpretations become philosophical rather than mathematical: different perspectives may motivate different constructions, but all constructions must conform to the same formal rules.

- **Generality and rigor.** The measure framework covers finite, countable, and continuous settings and brings powerful tools (integrals, convergence theorems, decomposition theorems) to probability.

- **Translation.** Frequentist long‑run limits, Bayesian priors/posteriors, geometric ratios, and subjective betting odds can be seen as particular measures, pushforward measures, or decision‑theoretic constructions on the same measurable space.

## 3. Probability theory as measure theory — core correspondences

**3.1 \(\sigma\)-algebra (events).** A collection \(\mathcal{F}\subseteq 2^\Omega\) closed under complements and countable unions. Elements of \(\mathcal{F}\) are precisely the events to which \(P\) assigns values.

**3.2 Probability measure.** A measure \(P\) on \(\mathcal{F}\) with total mass 1. All manipulations of probabilities reduce to measure theory operations.

**3.3 Measurable functions and random variables.** A random variable is a measurable map \(X:(\Omega,\mathcal{F})\to(\mathbb{R},\mathcal{B}(\mathbb{R}))\), i.e. for all Borel sets \(B\), \(X^{-1}(B)\in\mathcal{F}\). This guarantees events like \(\{X\in B\}\) are measurable.

**3.4 Distribution / pushforward measure.** The law of \(X\) is the pushforward measure \(\mu_X(B)=P(X\in B)=P(X^{-1}(B))\). Continuous and discrete laws are instances of such pushforwards.

**3.5 Expectation as Lebesgue integral.** For a measurable \(X\) with \(\int |X|\,dP<\infty\),

\[\mathbb{E}[X]=\int_\Omega X\,dP=\int_{\mathbb{R}} x\,d\mu_X(x)\]

This unifies sums (discrete) and integrals (continuous) and enables dominated/monotone convergence results.

**3.6 Conditional probability and expectation.** Conditioning on an event \(B\) of positive probability is the measure

\[P(A\mid B)=\frac{P(A\cap B)}{P(B)}\]

More generally, conditional expectation \(\mathbb{E}[X\mid\mathcal{G}]\) is the Radon–Nikodym derivative of a signed measure restricted to the sub‑sigma algebra \(\mathcal{G}\).

**3.7 Modes of convergence and limit theorems.** Measure theory supplies precise notions (almost sure, in probability, in \(L^p\), in distribution) and theorems (LLN, CLT) that rely on measure/integration tools.

## 4. Derivations from the axioms

### 4.1 Monotonicity (used repeatedly)

If \(A\subseteq B\) then write \(B=A\cup(B\setminus A)\) with disjoint union. By additivity and nonnegativity,

\[P(B)=P(A)+P(B\setminus A)\ge P(A)\]

### 4.2 Subadditivity (countable)

**Claim.** For any sequence \((A_i)_{i\ge1}\subseteq\mathcal{F}\),

\[P\Big(\bigcup_{i=1}^\infty A_i\Big)\le\sum_{i=1}^\infty P(A_i)\]

**Proof.** Define disjoint slices:

\[B_1=A_1,\quad B_2=A_2\setminus A_1,\quad B_3=A_3\setminus(A_1\cup A_2),\ \dots\]

Then \(B_i\cap B_j=\varnothing\) for \(i\ne j\) and \(\bigcup_i A_i=\bigcup_i B_i\). By countable additivity,

\[P\Big(\bigcup_i A_i\Big)=\sum_i P(B_i)\]

Monotonicity gives \(P(B_i)\le P(A_i)\) for each \(i\). Summing yields the inequality.

### 4.3 Inclusion–exclusion (finite case, full proof)

Let \(A_1,\dots,A_n\in\mathcal{F}\). Denote by \(1_A\) the indicator function of event \(A\). Pointwise identity on \(\Omega\):

\[1_{\bigcup_{i=1}^n A_i}=\sum_{k=1}^n(-1)^{k+1}\sum_{1\le i_1<\cdots<i_k\le n} 1_{A_{i_1}\cap\cdots\cap A_{i_k}}\]

This identity follows by fixing \(\omega\) and counting how many of the \(A_i\) contain \(\omega\): the alternating sum equals 1 if at least one set contains \(\omega\), and 0 otherwise.

Take expectation (integrate both sides w.r.t. \(P\)). Using linearity of expectation (which follows from countable additivity for simple functions) and \(\mathbb{E}[1_A]=P(A)\),

\[P\Big(\bigcup_{i=1}^n A_i\Big)=\sum_{k=1}^n(-1)^{k+1}\sum_{1\le i_1<\cdots<i_k\le n} P(A_{i_1}\cap\cdots\cap A_{i_k})\]

**Two‑set and three‑set examples.** For two sets, the formula reduces to

\[P(A\cup B)=P(A)+P(B)-P(A\cap B)\]

For three sets it yields

\[\begin{aligned}P(A\cup B\cup C)=&\;P(A)+P(B)+P(C)\\&-\big(P(A\cap B)+P(A\cap C)+P(B\cap C)\big)\\&+P(A\cap B\cap C)\end{aligned}\]

**Remarks.** Inclusion–exclusion as stated is a finite identity. For infinite collections, alternating series may fail to converge; one can obtain upper and lower bounds or take limits under convergence assumptions (e.g. monotone limits) but the finite formula is the safe canonical statement.

---

### Suggested concise references

Kolmogorov, *Foundations of the Theory of Probability*; Billingsley, *Probability and Measure*; Durrett, *Probability: Theory and Examples*.
